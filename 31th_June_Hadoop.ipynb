{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2f40d4",
   "metadata": {},
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444ce198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Components of Hadoop:\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "def display_hadoop_components(config_file):\n",
    "    # Create a ConfigParser object\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # Read the configuration file\n",
    "    config.read(config_file)\n",
    "\n",
    "    # Get the sections in the configuration file\n",
    "    sections = config.sections()\n",
    "\n",
    "    # Check if the sections contain Hadoop core components\n",
    "    core_components = ['namenode', 'datanode', 'secondarynamenode', 'resourcemanager', 'nodemanager']\n",
    "\n",
    "    # Display the core components of Hadoop\n",
    "    print(\"Core Components of Hadoop:\")\n",
    "    for section in sections:\n",
    "        if section.lower() in core_components:\n",
    "            print(section)\n",
    "\n",
    "# Specify the path to your Hadoop configuration file\n",
    "hadoop_config_file = '/path/to/hadoop/conf/hadoop-env.sh'\n",
    "\n",
    "# Call the function to display the core components\n",
    "display_hadoop_components(hadoop_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3a1e0",
   "metadata": {},
   "source": [
    "# 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_directory_size(hdfs_url, directory):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Get the file status of the directory\n",
    "    directory_status = client.status(directory)\n",
    "\n",
    "    # Check if the directory exists and is a directory\n",
    "    if directory_status['type'] != 'DIRECTORY':\n",
    "        print(f\"{directory} is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the total size of the directory\n",
    "    total_size = 0\n",
    "\n",
    "    # Get the file status of each file in the directory\n",
    "    file_statuses = client.list(directory, status=True)\n",
    "\n",
    "    for file_status in file_statuses:\n",
    "        if file_status['type'] == 'FILE':\n",
    "            total_size += file_status['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Specify the HDFS URL and the directory path\n",
    "hdfs_url = 'http://localhost:50070'\n",
    "directory_path = '/user/myuser/mydirectory'\n",
    "\n",
    "# Call the function to calculate the total file size\n",
    "total_size = calculate_directory_size(hdfs_url, directory_path)\n",
    "\n",
    "if total_size:\n",
    "    print(f\"Total file size in {directory_path}: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd53374",
   "metadata": {},
   "source": [
    "# 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63923853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--n', type=int, default=10, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_n)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_n(self, _, word_counts):\n",
    "        n = self.options.n\n",
    "        top_n = heapq.nlargest(n, word_counts)\n",
    "        for count, word in top_n:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d477c",
   "metadata": {},
   "source": [
    "# 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ea539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_namenode_health(namenode_url):\n",
    "    url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['beans'][0]['State']\n",
    "        \n",
    "        if state == 'active':\n",
    "            print(\"NameNode is active and healthy.\")\n",
    "        else:\n",
    "            print(\"NameNode is not active.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch NameNode status.\")\n",
    "\n",
    "def check_datanode_health(namenode_url):\n",
    "    url = f\"{namenode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        live_nodes = data['beans'][0]['LiveNodes']\n",
    "        dead_nodes = data['beans'][0]['DeadNodes']\n",
    "        \n",
    "        print(f\"Number of live DataNodes: {len(live_nodes)}\")\n",
    "        print(f\"Number of dead DataNodes: {len(dead_nodes)}\")\n",
    "        \n",
    "        if len(dead_nodes) == 0:\n",
    "            print(\"All DataNodes are healthy.\")\n",
    "        else:\n",
    "            print(\"Some DataNodes are dead.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch DataNode status.\")\n",
    "\n",
    "# Specify the URL of the NameNode's web interface\n",
    "namenode_url = 'http://localhost:9870'\n",
    "\n",
    "# Check the health status of the NameNode\n",
    "check_namenode_health(namenode_url)\n",
    "\n",
    "# Check the health status of the DataNodes\n",
    "check_datanode_health(namenode_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2d6c7",
   "metadata": {},
   "source": [
    "# 5. Develop a Python program that lists all the files and directories in a specific HDFS path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Check if the HDFS path exists\n",
    "    if not client.status(hdfs_path, strict=False):\n",
    "        print(f\"{hdfs_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # List all files and directories in the HDFS path\n",
    "    file_info = client.list(hdfs_path, status=True)\n",
    "\n",
    "    # Display the files and directories\n",
    "    print(f\"Files and Directories in {hdfs_path}:\")\n",
    "    for item in file_info:\n",
    "        item_path = item['path']\n",
    "        item_type = item['type']\n",
    "        item_size = item['length'] if item_type == 'FILE' else ''\n",
    "        print(f\"{item_type}: {item_path} ({item_size} bytes)\")\n",
    "\n",
    "# Specify the HDFS URL and the HDFS path\n",
    "hdfs_url = 'http://localhost:50070'\n",
    "hdfs_path = '/user/myuser/mydirectory'\n",
    "\n",
    "# Call the function to list the files and directories\n",
    "list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3cdbf",
   "metadata": {},
   "source": [
    "# 6. . Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69190acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def analyze_storage_utilization(hdfs_url):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Get the information of all DataNodes\n",
    "    datanodes_info = client.get_datanode_storage_report()\n",
    "\n",
    "    # Sort DataNodes based on storage capacity\n",
    "    sorted_datanodes = sorted(datanodes_info, key=lambda x: x['remaining'])\n",
    "\n",
    "    # Display the storage utilization for each DataNode\n",
    "    print(\"Storage Utilization of DataNodes:\")\n",
    "    for datanode in sorted_datanodes:\n",
    "        name = datanode['name']\n",
    "        used = datanode['used']\n",
    "        remaining = datanode['remaining']\n",
    "        capacity = datanode['capacity']\n",
    "        utilization = (used / capacity) * 100\n",
    "\n",
    "        print(f\"Node: {name}\")\n",
    "        print(f\"Storage Capacity: {capacity} bytes\")\n",
    "        print(f\"Used: {used} bytes\")\n",
    "        print(f\"Remaining: {remaining} bytes\")\n",
    "        print(f\"Utilization: {utilization:.2f}%\")\n",
    "        print()\n",
    "\n",
    "    # Identify the node with the highest and lowest storage capacities\n",
    "    highest_capacity_node = sorted_datanodes[-1]\n",
    "    lowest_capacity_node = sorted_datanodes[0]\n",
    "\n",
    "    print(\"Node with Highest Storage Capacity:\")\n",
    "    print(f\"Node: {highest_capacity_node['name']}\")\n",
    "    print(f\"Storage Capacity: {highest_capacity_node['capacity']} bytes\")\n",
    "    print()\n",
    "\n",
    "    print(\"Node with Lowest Storage Capacity:\")\n",
    "    print(f\"Node: {lowest_capacity_node['name']}\")\n",
    "    print(f\"Storage Capacity: {lowest_capacity_node['capacity']} bytes\")\n",
    "\n",
    "# Specify the HDFS URL\n",
    "hdfs_url = 'http://localhost:50070'\n",
    "\n",
    "# Call the function to analyze storage utilization\n",
    "analyze_storage_utilization(hdfs_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18bc141",
   "metadata": {},
   "source": [
    "# 7.  Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af36c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, jar_path, input_path, output_path):\n",
    "    # Submit the Hadoop job\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        app_id = data['application-id']\n",
    "        print(f\"Job submitted successfully. Application ID: {app_id}\")\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "        return\n",
    "\n",
    "    # Prepare the job configuration\n",
    "    job_config = {\n",
    "        \"application-id\": app_id,\n",
    "        \"application-name\": \"Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": f\"hadoop jar {jar_path} <input_args>\"\n",
    "            },\n",
    "            \"local-resources\": {\n",
    "                \"entry\": [\n",
    "                    {\n",
    "                        \"key\": \"input\",\n",
    "                        \"value\": input_path\n",
    "                    },\n",
    "                    {\n",
    "                        \"key\": \"output\",\n",
    "                        \"value\": output_path\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Submit the job configuration\n",
    "    job_submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps\"\n",
    "    response = requests.post(job_submit_url, json=job_config)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        print(\"Job configuration submitted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to submit the job configuration.\")\n",
    "        return\n",
    "\n",
    "    # Monitor job progress\n",
    "    while True:\n",
    "        job_status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}\"\n",
    "        response = requests.get(job_status_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            state = data['app']['state']\n",
    "            final_status = data['app']['finalStatus']\n",
    "\n",
    "            print(f\"Job state: {state}\")\n",
    "            print(f\"Final status: {final_status}\")\n",
    "\n",
    "            if state == 'FINISHED':\n",
    "                print(\"Job completed successfully.\")\n",
    "                break\n",
    "            elif state == 'FAILED' or state == 'KILLED':\n",
    "                print(\"Job failed or was killed.\")\n",
    "                break\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Retrieve the final output\n",
    "    final_output_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/appattempts\"\n",
    "    response = requests.get(final_output_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        app_attempt_id = data['appAttempts']['appAttempt'][0]['appAttemptId']\n",
    "        final_output = f\"{output_path}/{app_attempt_id}/container_*/stdout\"\n",
    "\n",
    "        print(\"Final output:\")\n",
    "        print(final_output)\n",
    "    else:\n",
    "        print(\"Failed to retrieve the final output.\")\n",
    "\n",
    "# Specify the URL of the YARN ResourceManager\n",
    "resource_manager_url = 'http://localhost:8088'\n",
    "\n",
    "# Specify the path to the Hadoop job JAR file, input path, and output path\n",
    "jar_path = '/path/to/hadoop-job.jar'\n",
    "input_path = '/input/path'\n",
    "output_path = '/output/path'\n",
    "\n",
    "# Call the function to submit the Hadoop job, monitor its progress, and retrieve the final output\n",
    "submit_hadoop_job(resource_manager_url, jar_path, input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b82ac",
   "metadata": {},
   "source": [
    "# 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, jar_path, input_path, output_path, num_containers, container_memory, container_vcores):\n",
    "    # Submit the Hadoop job\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        app_id = data['application-id']\n",
    "        print(f\"Job submitted successfully. Application ID: {app_id}\")\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "        return\n",
    "\n",
    "    # Prepare the job configuration\n",
    "    job_config = {\n",
    "        \"application-id\": app_id,\n",
    "        \"application-name\": \"Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": f\"hadoop jar {jar_path} <input_args>\"\n",
    "            },\n",
    "            \"local-resources\": {\n",
    "                \"entry\": [\n",
    "                    {\n",
    "                        \"key\": \"input\",\n",
    "                        \"value\": input_path\n",
    "                    },\n",
    "                    {\n",
    "                        \"key\": \"output\",\n",
    "                        \"value\": output_path\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"resource\": {\n",
    "            \"memory\": container_memory,\n",
    "            \"vCores\": container_vcores\n",
    "        },\n",
    "        \"instances\": num_containers\n",
    "    }\n",
    "\n",
    "    # Submit the job configuration\n",
    "    job_submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps\"\n",
    "    response = requests.post(job_submit_url, json=job_config)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        print(\"Job configuration submitted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to submit the job configuration.\")\n",
    "        return\n",
    "\n",
    "    # Monitor resource usage during job execution\n",
    "    while True:\n",
    "        job_status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/appattempts\"\n",
    "        response = requests.get(job_status_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            app_attempts = data['appAttempts']['appAttempt']\n",
    "            \n",
    "            if not app_attempts:\n",
    "                print(\"No application attempts found.\")\n",
    "                break\n",
    "\n",
    "            latest_attempt = app_attempts[-1]\n",
    "            latest_attempt_id = latest_attempt['appAttemptId']\n",
    "            container_report_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/appattempts/{latest_attempt_id}/containers\"\n",
    "            response = requests.get(container_report_url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                containers = data['containers']['container']\n",
    "                \n",
    "                if not containers:\n",
    "                    print(\"No containers found.\")\n",
    "                    break\n",
    "\n",
    "                print(\"Resource Usage:\")\n",
    "                for container in containers:\n",
    "                    container_id = container['id']\n",
    "                    container_state = container['state']\n",
    "                    container_memory = container['allocatedMB']\n",
    "                    container_vcores = container['allocatedVCores']\n",
    "\n",
    "                    print(f\"Container ID: {container_id}\")\n",
    "                    print(f\"State: {container_state}\")\n",
    "                    print(f\"Allocated Memory: {container_memory} MB\")\n",
    "                    print(f\"Allocated vCores: {container_vcores}\")\n",
    "                    print()\n",
    "\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(\"Failed to fetch container report.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to fetch job attempts.\")\n",
    "            break\n",
    "\n",
    "# Specify the URL of the YARN ResourceManager\n",
    "resource_manager_url = 'http://localhost:8088'\n",
    "\n",
    "# Specify the path to the Hadoop job JAR file, input path, and output path\n",
    "jar_path = '/path/to/hadoop-job.jar'\n",
    "input_path = '/input/path'\n",
    "output_path = '/output/path'\n",
    "\n",
    "# Specify the resource requirements\n",
    "num_containers = 2\n",
    "container_memory = 1024  # in MB\n",
    "container_vcores = 1\n",
    "\n",
    "# Call the function to submit the Hadoop job, set resource requirements, and track resource usage\n",
    "submit_hadoop_job(resource_manager_url, jar_path, input_path, output_path, num_containers, container_memory, container_vcores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa89e3",
   "metadata": {},
   "source": [
    "# 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class PerformanceComparison(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(PerformanceComparison, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, default=64, help='Input split size in MB')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Emit each word as a key with a count of 1\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        # Sum the counts of each word\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        # Output the total count for each word\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Specify the path to the input file\n",
    "    input_file = 'input.txt'\n",
    "\n",
    "    # Specify the split size in MB (change this to different split sizes for comparison)\n",
    "    split_size = 64\n",
    "\n",
    "    # Run the MapReduce job\n",
    "    mr_job = PerformanceComparison(args=[input_file, '--split-size', str(split_size)])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        # Print the word count results\n",
    "        for word, count in mr_job.parse_output(runner.cat_output()):\n",
    "            print(f\"{word}: {count}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print the execution time\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"\\nExecution time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130111d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f96ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42ade2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1707a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22085f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ad630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b91d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0668305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
